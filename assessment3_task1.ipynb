{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment3 task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Creating a Sentiment Prediction Platform (50/100) [max. 3 pages]\n",
    "We consider a real-life dataset consisting of 50,000 labeled gourmet food reviews from Amazon\n",
    "extracted from the work of McAuley and Leskovec1. A food review is labeled as 1 (positive) if it\n",
    "received four or more stars and 0 (negative) otherwise. The dataset is balanced and we provide two\n",
    "files (positive_reviews.csv and negative_reviews.csv) containing 25,000 positive and negative\n",
    "reviews respectively (see Figure 1).\n",
    "Review Label\n",
    "Over 6.00 a box and people call this a good deal?? Waste of money. You can get better\n",
    "deals at Sam's Club or Costco...\n",
    "0\n",
    "This was a very good buy and arrived in perfect condition in a very timely manner. Yes,\n",
    "I would order again.\n",
    "1\n",
    "Figure 1: Example of food reviews from the data\n",
    "The goal is to train machine learning (ML) models for sentiment prediction, evaluate them, and\n",
    "deploy the best model on a platform. The result should be similar to the service proposed on the\n",
    "following website: https://monkeylearn.com/sentiment-analysis-online/\n",
    "Subtasks:\n",
    "1.1) Using your own words, please explain the several steps that you will need to go through to\n",
    "create your sentiment prediction platform. Your description should include (but not only) the\n",
    "following points (2 marks):\n",
    "• Data retrieval\n",
    "• Feature Extraction\n",
    "• Feature Engineering\n",
    "• Model Evaluation\n",
    "• Deployment\n",
    "1.2) Please provide a short description of the dataset provided, along with how you imported the\n",
    "data, providing snippets of code along with a detailed description (2 marks).\n",
    "1.3) Employ exploratory data analysis (EDA) techniques to gain an initial understanding of the\n",
    "data. Please provide appropriate visualization results and initial insights gained from EDA\n",
    "(4 marks).\n",
    "1.4) Motivate, explain, and apply any necessary pre-processing techniques on your food reviews.\n",
    "Using an example, show how a string is transformed after each processing step. (6 marks)\n",
    "1.5) Implement the following techniques for sentiment prediction:\n",
    "• Logistic regression with BOW and TF-IDF word features\n",
    "• Support vector machine with BOW and TF-IDF word features\n",
    "• Long-short term memory network with an embedding layer\n",
    "For each of them, describe in detail how you deployed them and adjusted their parameters,\n",
    "going into detail on what each parameter does as well. You may use open source code and\n",
    "libraries as long as you acknowledge them (14 marks).\n",
    "1.6) Split your dataset into a training and test set of size 35,000 and 15,000 respectively. Train\n",
    "and evaluate the performance of the techniques developed in task 1.5. Please present and\n",
    "discuss your results using metrics and/or tables. (8 marks)\n",
    "page 4 of 4\n",
    "1.7) Select and save the “best” model, then deploy it on an online platform of your choice. The\n",
    "end-user should be able to input a string of text and receive its polarity (along with a\n",
    "confidence score). The report should contain the URL of the online platform along with\n",
    "detailed explanations and screenshots. You may use Flask or Django and services like\n",
    "Heroku. (14 marks)\n",
    "********************************\n",
    "Bonus – Optional: Should you decide to implement any further explanation features on your online\n",
    "platform along with the predicted sentiment, there will be a bonus of up to 5 marks. To explain the\n",
    "intuition of your explanation method and the code, you will be allowed at most one additional page.\n",
    "The maximum overall mark for this assessment remains at 100/100; however, attempting the bonus\n",
    "exercise will make you practice more on developing algorithms on your own and enhance your\n",
    "chances of getting a higher mark overall.\n",
    "********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Using cached contractions-0.0.43-py2.py3-none-any.whl (6.0 kB)\n",
      "Collecting textsearch\n",
      "  Using cached textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: pyahocorasick in c:\\programdata\\anaconda3\\envs\\tensorboard\\lib\\site-packages (from textsearch->contractions) (1.4.0)\n",
      "Requirement already satisfied: Unidecode in c:\\programdata\\anaconda3\\envs\\tensorboard\\lib\\site-packages (from textsearch->contractions) (1.1.1)\n",
      "Installing collected packages: textsearch, contractions\n",
      "Successfully installed contractions-0.0.43 textsearch-0.0.17\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting afinn\n",
      "  Downloading afinn-0.1.tar.gz (52 kB)\n",
      "Building wheels for collected packages: afinn\n",
      "  Building wheel for afinn (setup.py): started\n",
      "  Building wheel for afinn (setup.py): finished with status 'done'\n",
      "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53449 sha256=bc60bf6f92809587635d26d3dbe337e8c960b54ed73fe8d9e5b71200d1d246dd\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\9d\\16\\3a\\9f0953027434eab5dadf3f33ab3298fa95afa8292fcf7aba75\n",
      "Successfully built afinn\n",
      "Installing collected packages: afinn\n",
      "Successfully installed afinn-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual data representation and manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK is very useful for natural language applications\n",
    "import nltk\n",
    "\n",
    "# This will be used to tokenize sentences\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "# We use spacy for extracting useful information from English words\n",
    "import spacy\n",
    "nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
    "\n",
    "# This dictionary will be used to expand contractions (e.g. we'll -> we will)\n",
    "from contractions import contractions_dict\n",
    "import re\n",
    "\n",
    "# Unicodedata will be used to remove accented characters\n",
    "import unicodedata\n",
    "\n",
    "# BeautifulSoup will be used to remove html tags\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Lexicon models\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Label\n",
      "0  We love Malibu Rum but they sure missed the ma...      0\n",
      "1  I just wanted to say that if you want to get y...      0\n",
      "2  These seeds were accompanied by small broken p...      0\n",
      "3  Way way way overpriced. I can get this same se...      0\n",
      "4  I bought these on the strength of the reviews....      0 (25000, 2)\n",
      "                                              Review  Label\n",
      "0  Better than Wolff's Kasha. I grew up eating Ka...      1\n",
      "1  It was such good product. Came in two differen...      1\n",
      "2  MMMM Yes all chocolate is good.<br />But some ...      1\n",
      "3  This is, as all of their cereals I've ordered ...      1\n",
      "4  Whoever Photoshopped the cookie on the front o...      1 (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "#We import the dataset\n",
    "negative_reviews_raw = pd.read_csv(\"D:/MScAI/Semester1/CS5079AppliedAI/Assessm3/negative.csv\")\n",
    "positive_reviews_raw = pd.read_csv(\"D:/MScAI/Semester1/CS5079AppliedAI/Assessm3/positive.csv\")\n",
    "print(negative_reviews_raw.head(), negative_reviews_raw.shape)\n",
    "print(positive_reviews_raw.head(), positive_reviews_raw.shape)                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Better than Wolff's Kasha. I grew up eating Ka...\n",
       "1        It was such good product. Came in two differen...\n",
       "2        MMMM Yes all chocolate is good.<br />But some ...\n",
       "3        This is, as all of their cereals I've ordered ...\n",
       "4        Whoever Photoshopped the cookie on the front o...\n",
       "                               ...                        \n",
       "24995    Healthy alternative, high in protein, especial...\n",
       "24996    I have tried a half a dozen unsweetened drinks...\n",
       "24997    Is a little bit more costly then other dog foo...\n",
       "24998    My husband is from Hungary and craves foods th...\n",
       "24999    Katy Perry has sung the praises of <a href=\"ht...\n",
       "Name: Review, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews_raw['Review'] # negative_reviews_raw.rename(columns = lambda x: x.replace(' ', '_'), inplace=True)\n",
    "# negative_reviews_raw.info()\n",
    "# negative_reviews_raw.columns = negative_reviews_raw.columns.str.strip()\n",
    "# negative_reviews_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  25000 non-null  object\n",
      " 1   Label   25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  25000 non-null  object\n",
      " 1   Label   25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "negative_reviews_raw.info()\n",
    "positive_reviews_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 25000\n",
       "unique                                                23755\n",
       "top       I'm addicted to salty and tangy flavors, so wh...\n",
       "freq                                                      6\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews_raw['Review'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 25000\n",
       "unique                                                21682\n",
       "top       This review will make me sound really stupid, ...\n",
       "freq                                                     29\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_reviews_raw['Review'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10112\n",
      "9296\n"
     ]
    }
   ],
   "source": [
    "print (positive_reviews_raw.Review.map(lambda x: len(x)).max())\n",
    "print (negative_reviews_raw.Review.map(lambda x: len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420.4514\n",
      "495.21012\n"
     ]
    }
   ],
   "source": [
    "print (positive_reviews_raw.Review.map(len).mean())\n",
    "print (negative_reviews_raw.Review.map(len).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print (positive_reviews_raw.Review.map(len).min())\n",
    "print (negative_reviews_raw.Review.map(len).min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MMMM Yes all chocolate is good.<br />But some chocolates are better than others and this is a \"better than others chocolate\"<br />It has a smooth, easy creamy taste not overly sweet IMO.<br />It is just pretty much perfect'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews_raw.Review[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())                               \n",
    "        return first_char+expanded_contraction[1:] if expanded_contraction != None else match\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    return ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    i=0\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        i+=1\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "            if i==3: print(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "            if i==3: print(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "            if i==3: print(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "            if i==3: print(doc)\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        if i==3: print(doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        if i==3: print(doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "            if i==3: print(doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)\n",
    "            if i==3: print(doc)\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        if i==3: print(doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            if i==3: print(doc)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMMM Yes all chocolate is good.But some chocolates are better than others and this is a \"better than others chocolate\"It has a smooth, easy creamy taste not overly sweet IMO.It is just pretty much perfect\n",
      "MMMM Yes all chocolate is good.But some chocolates are better than others and this is a \"better than others chocolate\"It has a smooth, easy creamy taste not overly sweet IMO.It is just pretty much perfect\n",
      "MMMM Yes all chocolate is good.But some chocolates are better than others and this is a \"better than others chocolate\"It has a smooth, easy creamy taste not overly sweet IMO.It is just pretty much perfect\n",
      "mmmm yes all chocolate is good.but some chocolates are better than others and this is a \"better than others chocolate\"it has a smooth, easy creamy taste not overly sweet imo.it is just pretty much perfect\n",
      "mmmm yes all chocolate is good.but some chocolates are better than others and this is a \"better than others chocolate\"it has a smooth, easy creamy taste not overly sweet imo.it is just pretty much perfect\n",
      "mmmm yes all chocolate is good . but some chocolates are better than others and this is a \"better than others chocolate\"it has a smooth, easy creamy taste not overly sweet imo . it is just pretty much perfect\n",
      "mmmm yes all chocolate be good . but some chocolate be well than other and this be a \" well than other chocolate\"it have a smooth , easy creamy taste not overly sweet imo . it be just pretty much perfect\n",
      "mmmm yes all chocolate be good  but some chocolate be well than other and this be a  well than other chocolateit have a smooth  easy creamy taste not overly sweet imo  it be just pretty much perfect\n",
      "mmmm yes all chocolate be good but some chocolate be well than other and this be a well than other chocolateit have a smooth easy creamy taste not overly sweet imo it be just pretty much perfect\n",
      "mmmm yes chocolate good chocolate well well chocolateit smooth easy creamy taste not overly sweet imo pretty much perfect\n",
      "These seeds were accompanied by small broken pieces of seed that just make the bag weigh 1 pound but the water is cloudy at rinsing and doesn't germinate.  They sift their way to the bottom of the barrel and should be thrown out.  They are included to weigh a full pound with pieces that should not be included. I have been growing wheatgrass for over 15 years.  I will not buy these seeds again.\n",
      "These seeds were accompanied by small broken pieces of seed that just make the bag weigh 1 pound but the water is cloudy at rinsing and doesn't germinate.  They sift their way to the bottom of the barrel and should be thrown out.  They are included to weigh a full pound with pieces that should not be included. I have been growing wheatgrass for over 15 years.  I will not buy these seeds again.\n",
      "These seeds were accompanied by small broken pieces of seed that just make the bag weigh 1 pound but the water is cloudy at rinsing and does not germinate.  They sift their way to the bottom of the barrel and should be thrown out.  They are included to weigh a full pound with pieces that should not be included. I have been growing wheatgrass for over 15 years.  I will not buy these seeds again.\n",
      "these seeds were accompanied by small broken pieces of seed that just make the bag weigh 1 pound but the water is cloudy at rinsing and does not germinate.  they sift their way to the bottom of the barrel and should be thrown out.  they are included to weigh a full pound with pieces that should not be included. i have been growing wheatgrass for over 15 years.  i will not buy these seeds again.\n",
      "these seeds were accompanied by small broken pieces of seed that just make the bag weigh 1 pound but the water is cloudy at rinsing and does not germinate.  they sift their way to the bottom of the barrel and should be thrown out.  they are included to weigh a full pound with pieces that should not be included. i have been growing wheatgrass for over 15 years.  i will not buy these seeds again.\n",
      "these seeds were accompanied by small broken pieces of seed that just make the bag weigh 1 pound but the water is cloudy at rinsing and does not germinate .   they sift their way to the bottom of the barrel and should be thrown out .   they are included to weigh a full pound with pieces that should not be included .  i have been growing wheatgrass for over 15 years .   i will not buy these seeds again . \n",
      "these seed be accompany by small broken piece of seed that just make the bag weigh 1 pound but the water be cloudy at rinse and do not germinate .    they sift their way to the bottom of the barrel and should be throw out .    they be include to weigh a full pound with piece that should not be include .   i have be grow wheatgrass for over 15 year .    i will not buy these seed again .\n",
      "these seed be accompany by small broken piece of seed that just make the bag weigh 1 pound but the water be cloudy at rinse and do not germinate     they sift their way to the bottom of the barrel and should be throw out     they be include to weigh a full pound with piece that should not be include    i have be grow wheatgrass for over 15 year     i will not buy these seed again \n",
      "these seed be accompany by small broken piece of seed that just make the bag weigh 1 pound but the water be cloudy at rinse and do not germinate they sift their way to the bottom of the barrel and should be throw out they be include to weigh a full pound with piece that should not be include i have be grow wheatgrass for over 15 year i will not buy these seed again \n",
      "seed accompany small broken piece seed make bag weigh 1 pound water cloudy rinse not germinate sift way bottom barrel throw include weigh full pound piece not include grow wheatgrass 15 year not buy seed\n"
     ]
    }
   ],
   "source": [
    "# positive_reviews_raw['Review'] = normalize_corpus(positive_reviews_raw.Review)\n",
    "# positive_reviews_raw.to_csv(\"normalized_positive_reviews.csv\", index = False)\n",
    "# negative_reviews_raw['Review'] = normalize_corpus(negative_reviews_raw.Review)\n",
    "# negative_reviews_raw.to_csv(\"normalized_negative_reviews.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The train and test data sets creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_positive_reviews = pd.read_csv(\"normalized_positive_reviews.csv\")\n",
    "normalized_negative_reviews = pd.read_csv(\"normalized_negative_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000,) (15000,)\n",
      "(35000,) (35000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        always favorite coffee cremor make coffee smoo...\n",
       "1        cat discerning palate eat fancy feast fragrant...\n",
       "2        first attempt gluten free bread make regular b...\n",
       "3        fiasconaro panettone make wonderful christmas ...\n",
       "4        sparkling blackberry not get cola hope nice dr...\n",
       "                               ...                        \n",
       "34995    many glowing review feel little ashamed put lu...\n",
       "34996    give 3 star purpose bake mix really good panca...\n",
       "34997    affordable bully stick office fill incredible ...\n",
       "34998    get daughter love blood hot chocolate get stuf...\n",
       "34999    not enough flavor murky brown color lack aroma...\n",
       "Name: Review, Length: 35000, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df = pd.concat([normalized_positive_reviews.Review.iloc[:7500],\n",
    "                       normalized_negative_reviews.Review.iloc[:7500]],\n",
    "                       ignore_index=True)\n",
    "y_test_df = pd.concat([normalized_positive_reviews.Label.iloc[:7500],\n",
    "                       normalized_negative_reviews.Label.iloc[:7500]], \n",
    "                       ignore_index=True)\n",
    "X_train_df = pd.concat([normalized_positive_reviews.Review.iloc[7500:42500],\n",
    "                        normalized_negative_reviews.Review.iloc[7500:42500]],\n",
    "                        ignore_index=True)\n",
    "y_train_df = pd.concat([normalized_positive_reviews.Label.iloc[7500:42500],\n",
    "                        normalized_negative_reviews.Label.iloc[7500:42500]],\n",
    "                        ignore_index=True)\n",
    "\n",
    "print(X_test_df.shape, y_test_df.shape)\n",
    "print(X_train_df.shape, y_train_df.shape)\n",
    "X_train_df=X_train_df.astype('str')\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n",
      "35000 35000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['well wolffs kasha grow eat kasha easy prepare healthy meal bobs red mill great selection thing grain ever portland area check store restaurant great vegetarian selection well meat eater stuff',\n",
       "       'good product come two different box describe cover plastic wrap together',\n",
       "       'mmmm yes chocolate good chocolate well well chocolateit smooth easy creamy taste not overly sweet imo pretty much perfect',\n",
       "       ...,\n",
       "       'three dog love greenie one dog not chew food gulps whole cut greenie small portion help clean tooth cause diarrhea think look another product tooth clean',\n",
       "       'buy 5 bag think would great work energy protein mineral content first bag open okay could tolerate shake sprinkle food hemp protein powder open next bag share friend horrify bitter nasty taste not know get bad batch stuff would try one bag becausew stick 5 bag',\n",
       "       'vanilla coffee taste pretty good mild expect buy discount amazon warehouse otherwise k cup little expensive amazon usually mean try get rid stock quickly expect expiration date month two try get rid also notice seal not seem tight normally also could sit amazon warehouse heat summer expect half k cup burst open brew not usual try many different brand make leak clean kind annoying not taste fresh still taste decent bottom line k cup discount amazon warehouse ready pay way lol'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(X_test_df)\n",
    "y_test = np.array(y_test_df)\n",
    "X_train = np.array(X_train_df)\n",
    "y_train = np.array(y_train_df)\n",
    "print (len(X_test), len(y_test))\n",
    "print (len(X_train), len(y_train))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "afn = Afinn(emoticons=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = lambda x: 1 if x else 0\n",
    "y_predicted = [T(x) for x in [afn.score(review)>=0 for review in X_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.6056\n",
      "The model precision score is: 0.7146707276296839\n",
      "The model recall score is: 0.6056\n",
      "The model F1-score is: 0.5482136661608737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.25      0.39      7500\n",
      "           1       0.56      0.96      0.71      7500\n",
      "\n",
      "    accuracy                           0.61     15000\n",
      "   macro avg       0.71      0.61      0.55     15000\n",
      "weighted avg       0.71      0.61      0.55     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>1869</td>\n",
       "      <td>5631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>285</td>\n",
       "      <td>7215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            1869            5631\n",
       "Act. positive             285            7215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The model accuracy score is: {}\".format(accuracy_score(y_test, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(y_test, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordNet\n",
    "\n",
    "Wordnet groupes synonyms into synsets with short definitions and usage examples. In the example below, we print the synsets for the word `extravagant`. You can notice than each synset is associated with a positive, a negative and an objectivity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Synset</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Positive Polarity</th>\n",
       "      <th>Negative Polarity</th>\n",
       "      <th>Objectivity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Synset('excessive.s.02')</td>\n",
       "      <td>unrestrained, especially with regard to feelings</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Synset('extravagant.s.02')</td>\n",
       "      <td>recklessly wasteful</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Synset  \\\n",
       "0    Synset('excessive.s.02')   \n",
       "1  Synset('extravagant.s.02')   \n",
       "\n",
       "                                         Definition  Positive Polarity  \\\n",
       "0  unrestrained, especially with regard to feelings              0.125   \n",
       "1                               recklessly wasteful              0.000   \n",
       "\n",
       "   Negative Polarity  Objectivity Score  \n",
       "0              0.375              0.500  \n",
       "1              0.125              0.875  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extravant = list(swn.senti_synsets('extravagant', 'a'))\n",
    "pd.DataFrame.from_dict({ \"Synset\" : [ s.synset for s in extravant],\n",
    "\"Definition\" : [s.synset.definition() for s in extravant], \"Positive Polarity\" : [s._pos_score for s in extravant], \"Negative Polarity\" : [s._neg_score for s in extravant], \"Objectivity Score\" : [s._obj_score for s in extravant]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_sentiwordnet_lexicon(review):\n",
    "    # tokenize and POS tag text tokens\n",
    "    tagged_text = [(token.text, token.tag_) for token in nlp(review)]\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    # get wordnet synsets based on POS tags\n",
    "    # get sentiment scores if synsets are found\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')): #NOUNS\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
    "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')): #VERBS\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
    "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')): #ADJECTIVES\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
    "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')): #ADVERBS\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
    "        # if senti-synset is found        \n",
    "        if ss_set:\n",
    "            # add scores for all found synsets\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    # aggregate final scores\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    return norm_final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: not say like soup not say dislike kind culinary non event not flavor speak potato good base though want dress add broccoli cheese little chicken stock\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: -0.08\n",
      "------------------------------------------------------------\n",
      "REVIEW: use morsel bake particularly baking banana breads standard price conducive\n",
      "Actual Sentiment: 1\n",
      "Predicted Sentiment polarity: 0.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: kind goody excited try learn quickly need sweetener eat plain oatmeal cocoa stuff pretty awful without something sweet eat work cheat use flavor coffee creamer instantly make quantity stuff pretty yummy good taste good make good stuff pretty happy wheat free cap front along vegan superfood marketing word note make facility also process wheat not sensitive sensitive not die explode good big complaint stuff nut oil seem little stale hate rancid oil pick mile away picky people would parent old eat kind thing consider past prime expiration date chocolate hot cereal let us call oatmeal lot fun stuff mix year future always store sensibly not crazy quality ingredient know ingredient list awesome really suffer bad storage old age maybe get sub par crop mix review base one stuff perhaps experience unusual impossible tell without buy second base far go smell one not sure go happen time soon familiar rancid oil nut past prime different experience batch stuff get toward end would love hear kind food really want like convenient work breakfast full ingredient healthy delicious\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 0.02\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sample_review_ids = [7626, 3533, 13010]\n",
    "\n",
    "for review, sentiment in zip(X_test[sample_review_ids], y_test[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity: '+ str(analyze_sentiment_sentiwordnet_lexicon(review)))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = [T(x) for x in [analyze_sentiment_sentiwordnet_lexicon(review)>=0 for review in X_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.6779333333333334\n",
      "The model precision score is: 0.682149092310148\n",
      "The model recall score is: 0.6779333333333334\n",
      "The model F1-score is: 0.6760589658780278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.60      0.65      7500\n",
      "           1       0.65      0.75      0.70      7500\n",
      "\n",
      "    accuracy                           0.68     15000\n",
      "   macro avg       0.68      0.68      0.68     15000\n",
      "weighted avg       0.68      0.68      0.68     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>4514</td>\n",
       "      <td>2986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>1845</td>\n",
       "      <td>5655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            4514            2986\n",
       "Act. positive            1845            5655"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The model accuracy score is: {}\".format(accuracy_score(y_test, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(y_test, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. It is available in the NLTK package and can be applied directly to unlabeled text data.\n",
    "\n",
    "In the next cell, we can see that VADER returns four sentiment scores `compound`, `neg`, `neu` and `pos`. In the following model, we will only use the `compound` (i.e. the aggregated score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.41, 'neu': 0.59, 'pos': 0.0, 'compound': -0.6759}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "analyzer.polarity_scores('This movie was actually neither that funny, nor super witty.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_vader_lexicon(review, threshold=0.1):\n",
    "    # analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # get aggregate scores and final sentiment\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 'positive' if agg_score >= threshold else 'negative'\n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: not say like soup not say dislike kind culinary non event not flavor speak potato good base though want dress add broccoli cheese little chicken stock\n",
      "Actual Sentiment: 0\n",
      "------------------------------------------------------------\n",
      "REVIEW: use morsel bake particularly baking banana breads standard price conducive\n",
      "Actual Sentiment: 1\n",
      "------------------------------------------------------------\n",
      "REVIEW: kind goody excited try learn quickly need sweetener eat plain oatmeal cocoa stuff pretty awful without something sweet eat work cheat use flavor coffee creamer instantly make quantity stuff pretty yummy good taste good make good stuff pretty happy wheat free cap front along vegan superfood marketing word note make facility also process wheat not sensitive sensitive not die explode good big complaint stuff nut oil seem little stale hate rancid oil pick mile away picky people would parent old eat kind thing consider past prime expiration date chocolate hot cereal let us call oatmeal lot fun stuff mix year future always store sensibly not crazy quality ingredient know ingredient list awesome really suffer bad storage old age maybe get sub par crop mix review base one stuff perhaps experience unusual impossible tell without buy second base far go smell one not sure go happen time soon familiar rancid oil nut past prime different experience batch stuff get toward end would love hear kind food really want like convenient work breakfast full ingredient healthy delicious\n",
      "Actual Sentiment: 0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(X_test[sample_review_ids], y_test[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4)    \n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.5\n",
      "The model precision score is: 0.25\n",
      "The model recall score is: 0.5\n",
      "The model F1-score is: 0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      7500\n",
      "           1       0.50      1.00      0.67      7500\n",
      "\n",
      "    accuracy                           0.50     15000\n",
      "   macro avg       0.25      0.50      0.33     15000\n",
      "weighted avg       0.25      0.50      0.33     15000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensorboard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensorboard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>0</td>\n",
       "      <td>7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>0</td>\n",
       "      <td>7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative               0            7500\n",
       "Act. positive               0            7500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predicted = [T(x) for x in [analyze_sentiment_vader_lexicon(review, threshold=0.4) for review in X_test]]\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(y_test, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(y_test, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predicted),\n",
    "                     columns=[\"Pred. negative\", \"Pred. positive\"],\n",
    "                     index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bag of Words (BOW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for feature engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (35000, 623304)  Test features shape: (15000, 623304)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our SVM and LR models\n",
    "lr = LogisticRegression(penalty='l2', max_iter=1000, C=1)\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.876\n",
      "The model precision score is: 0.876000240640154\n",
      "The model recall score is: 0.876\n",
      "The model F1-score is: 0.8759999801599969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      7500\n",
      "           1       0.88      0.88      0.88      7500\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>6573</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>933</td>\n",
       "      <td>6567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            6573             927\n",
       "Act. positive             933            6567"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression model on BOW features\n",
    "lr.fit(cv_train_features,y_train)\n",
    "y_predicted = lr.predict(cv_test_features)\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(y_test, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(y_test, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.8690666666666667\n",
      "The model precision score is: 0.8691519555344733\n",
      "The model recall score is: 0.8690666666666667\n",
      "The model F1-score is: 0.869059103520486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      7500\n",
      "           1       0.87      0.86      0.87      7500\n",
      "\n",
      "    accuracy                           0.87     15000\n",
      "   macro avg       0.87      0.87      0.87     15000\n",
      "weighted avg       0.87      0.87      0.87     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>6575</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>1039</td>\n",
       "      <td>6461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            6575             925\n",
       "Act. positive            1039            6461"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM model on BOW\n",
    "svm.fit(cv_train_features,y_train)\n",
    "y_predicted = svm.predict(cv_test_features)\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(y_test, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(y_test, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_test_features = tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF model:> Train features shape: (35000, 623304)  Test features shape: (15000, 623304)\n"
     ]
    }
   ],
   "source": [
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.8651333333333333\n",
      "The model precision score is: 0.8652661213673928\n",
      "The model recall score is: 0.8651333333333333\n",
      "The model F1-score is: 0.8651210749371617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87      7500\n",
      "           1       0.87      0.86      0.86      7500\n",
      "\n",
      "    accuracy                           0.87     15000\n",
      "   macro avg       0.87      0.87      0.87     15000\n",
      "weighted avg       0.87      0.87      0.87     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>6560</td>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>1083</td>\n",
       "      <td>6417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            6560             940\n",
       "Act. positive            1083            6417"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features\n",
    "lr.fit(tv_train_features,y_train)\n",
    "y_predicted = lr.predict(tv_test_features)\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(y_test, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(y_test, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.8652\n",
      "The model precision score is: 0.865370467245198\n",
      "The model recall score is: 0.8652\n",
      "The model F1-score is: 0.865184275093847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      7500\n",
      "           1       0.87      0.85      0.86      7500\n",
      "\n",
      "    accuracy                           0.87     15000\n",
      "   macro avg       0.87      0.87      0.87     15000\n",
      "weighted avg       0.87      0.87      0.87     15000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>6570</td>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>1092</td>\n",
       "      <td>6408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            6570             930\n",
       "Act. positive            1092            6408"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SVM model on TF-IDF\n",
    "svm.fit(tv_train_features,y_train)\n",
    "y_predicted = svm.predict(tv_test_features)\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(y_test, y_predicted)))\n",
    "print(\"The model precision score is: {}\".format(precision_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(y_test, y_predicted, average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(y_test, y_predicted, average=\"weighted\")))\n",
    "\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predicted), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-short term memory network (LSTM) with an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 1000) (35000, 2)\n",
      "(15000, 1000) (15000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Truncate and pad the review sequences \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "max_features = 1000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_token = pad_sequences(X_train_token, maxlen=max_features) \n",
    "\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_token = pad_sequences(X_test_token, maxlen=max_features)\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)\n",
    "\n",
    "print(X_train_token.shape,Y_train.shape)\n",
    "print(X_test_token.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 1000, 64)          64000     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 200)               212000    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 276,402\n",
      "Trainable params: 276,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "lstm_out = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train_token.shape[1]))\n",
    "model.add(LSTM(lstm_out)) \n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "35000/35000 [==============================] - 1479s 42ms/step - loss: 0.4524 - accuracy: 0.7930\n",
      "Epoch 2/5\n",
      "35000/35000 [==============================] - 1645s 47ms/step - loss: 0.3856 - accuracy: 0.8325\n",
      "Epoch 3/5\n",
      "35000/35000 [==============================] - 1784s 51ms/step - loss: 0.3686 - accuracy: 0.8429\n",
      "Epoch 4/5\n",
      "35000/35000 [==============================] - 1710s 49ms/step - loss: 0.3495 - accuracy: 0.8508\n",
      "Epoch 5/5\n",
      "35000/35000 [==============================] - 1748s 50ms/step - loss: 0.3344 - accuracy: 0.8556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x222a8de39c8>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit(X_train_token, Y_train, epochs = 5, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LSTM model prediction on test data\n",
    "y_pred_lstm = model.predict(X_test_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy score is: 0.8388\n",
      "The model precision score is: 0.8394841426340026\n",
      "The model recall score is: 0.8402666666666667\n",
      "The model F1-score is: 0.8398740152096007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      7500\n",
      "           1       0.84      0.84      0.84      7500\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     15000\n",
      "   macro avg       0.84      0.84      0.84     15000\n",
      "weighted avg       0.84      0.84      0.84     15000\n",
      " samples avg       0.84      0.84      0.84     15000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensorboard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred. negative</th>\n",
       "      <th>Pred. positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Act. negative</th>\n",
       "      <td>6297</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Act. positive</th>\n",
       "      <td>1200</td>\n",
       "      <td>6300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pred. negative  Pred. positive\n",
       "Act. negative            6297            1203\n",
       "Act. positive            1200            6300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#LSTM model evaluation\n",
    "\n",
    "print(\"The model accuracy score is: {}\".format(accuracy_score(Y_test, y_pred_lstm.round())))\n",
    "print(\"The model precision score is: {}\".format(precision_score(Y_test, y_pred_lstm.round(), average=\"weighted\")))\n",
    "print(\"The model recall score is: {}\".format(recall_score(Y_test, y_pred_lstm.round(), average=\"weighted\")))\n",
    "print(\"The model F1-score is: {}\".format(f1_score(Y_test, y_pred_lstm.round(), average=\"weighted\")))\n",
    "\n",
    "print(classification_report(Y_test, y_pred_lstm.round()))\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test,np.argmax(y_pred_lstm.round(), axis=1)), columns=[\"Pred. negative\", \"Pred. positive\"], index=[\"Act. negative\", \"Act. positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
